{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d67488f8-e9bb-445b-a0f0-8ef602326258",
   "metadata": {},
   "source": [
    "# Unlocking Financial Frontiers: A Deep Dive into Revolutionizing Trading with Pretrained Deep Learning Models and Python Bots\n",
    "\n",
    "#### by Atanas Vasev"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2f251d-cac4-44eb-9ebf-4fede498a5e9",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This term paper introduces a pioneering method in the domain of trading, harnessing the capabilities of pretrained deep learning models intricately designed for the complexities of finance. Through the deliberate crafting of a strategic blueprint, the paper outlines the seamless integration of this intelligence into a Python-driven trading bot, subjected to meticulous backtesting using authentic market data. The primary objective is to unveil the transformative potential of deep learning, offering traders liberation from the tedious routine of perpetual financial news analysis.\n",
    "\n",
    "Given the nature of this paper, situated within a Deep Learning course, it is imperative to acknowledge the formidable challenges in developing the underlying architecture of these models. The computational demands for training from scratch pose significant obstacles in terms of time and cost. While tools such as Google Colab offer a solution, the complexity of creating a neural network from scratch necessitates a judicious approach.\n",
    "\n",
    "This leads us to the pragmatic choice of utilizing pretrained models. Acknowledging the challenges of constructing architectures from the ground up, this approach aims not only to streamline trading processes but also to enhance overall quality of life. By aligning with the fundamental principles of scientific and general progress, this term paper seeks to make our existence not only more efficient and productive but also undeniably better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563b0c39-1144-4dfd-aae8-cf02cd7f48a6",
   "metadata": {},
   "source": [
    "# 1 BERT - Bidirectional Encoder Representation from Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e302974f-ad70-4f18-aaec-6059ebf2f995",
   "metadata": {},
   "source": [
    "BERT enables context awareness for sentences. It is one of the most popular state of the art text embedding model published by Google. BERT has caused a revolution in the world of NLP by providing superior results on many NLP tasks, such as question answering, text generation, sentence classification, and many more compared to other methods.One of the reasons BERT is more successful is that it uses a context based embedding model. \n",
    "\n",
    "Consider the example below:\n",
    "\n",
    "**Sentence 1: The python ate the rabbit**\n",
    "\n",
    "**Sentence 2: Python is one of the most popular programming languages**\n",
    "\n",
    "Without context, the word python would have the same meaning in both sentences. BERT looks at the sentence and figures out what words python is related to in the sentence, and will create embedding of the word python based on the context. BERT does this by using transformers, which is a state of the art deep learning architecture, that is mostly used for Natural language processing. The architecture uses encoder-decoder paradigm.\n",
    "\n",
    "The encoder takes the input sentence and learns its representation and then sends the representation to the decoder. The decoder generates the output sentence. The transformer architecture uses many layers of encoders to generate the representation. BERT can be thought of as a transformer, but only with encoders. BERT has different configurations based on how many encoder layers it uses.\n",
    "\n",
    "<img src=\"./IMG/BERT_Arhitecture.PNG\" width=\"800\">\n",
    "\n",
    "The BERT model is pretrained on a large corpus of words. What is pretraining? Pretraining is when we train a model with a huge dataset and potentially a very large number of parameters for a particular task and save the trained model. For any new task, instead of initializing a new model with random weights, we will initialize it with the weights of the trained model, and adjust the weights for the new task. This is helpful since for our work we may not have easy access to huge volumes of training data, and we will save a lot of time and resources that were spent on training the model. \n",
    "\n",
    "The BERT model is pre-trained using two tasks: masked language modelling and next sentence prediction. BERT models have been trained on BookCorpus and English Wikipedia, which have in total more than 3.5 Billion words. Many domain specific models have emerged using BERT as the base and are being used for NLP tasks. Some of them are: FinBERT for Finance, BioBERT for Biomedical, VideoBERT for Video captioning categorization, ClinicalBERT for hospitals, and many more continue to evolve. If you are looking for cutting edge, deep learning pre-trained models for any domain, it would be worth researching to see if a DomainBERT model for that area exists.\n",
    "\n",
    "### 1.1 FinBERT\n",
    "\n",
    "FinBERT is a language model based on BERT. It further trains the BERT model for financial data. The additional training corpus is a set of 1.8M Reuters’ news articles and Financial PhraseBank.The main sentiment analysis dataset used is Financial PhraseBank which consists of 4845 English sentences selected randomly from financial news found on LexisNexis database. These sentences then were annotated by 16 people with backgrounds in finance and business.\n",
    "\n",
    "<img src=\"./IMG/FinBERT.PNG\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23317e77-0314-42b5-811f-dacb2a35f374",
   "metadata": {},
   "source": [
    "### 1.2 Brief implementation\n",
    "\n",
    "FinBERT implementation is reliant on Hugging Face’s pytorch_pretrained_bert library and their implementation of BERT for sequence classification tasks.In order to demonstrate FinBert in action, I will use a financial news dataset from [Kaggle](https://www.kaggle.com/datasets/notlucasp/financial-news-headlines)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d3c3dc-d6ae-424d-8590-05703517612a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6adba49a-c461-4373-91bd-12f55516aad9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
