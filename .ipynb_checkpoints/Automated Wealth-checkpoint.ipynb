{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d67488f8-e9bb-445b-a0f0-8ef602326258",
   "metadata": {},
   "source": [
    "# Unlocking Financial Frontiers: A Deep Dive into Revolutionizing Trading with Pretrained Deep Learning Models and Python Bots\n",
    "\n",
    "#### by Atanas Vasev"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2f251d-cac4-44eb-9ebf-4fede498a5e9",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This term paper introduces a pioneering method in the domain of trading, harnessing the capabilities of pretrained deep learning models intricately designed for the complexities of finance. Through the deliberate crafting of a strategic blueprint, the paper outlines the seamless integration of this intelligence into a Python-driven trading bot, subjected to meticulous backtesting using authentic market data. The primary objective is to unveil the transformative potential of deep learning, offering traders liberation from the tedious routine of perpetual financial news analysis.\n",
    "\n",
    "Given the nature of this paper, situated within a Deep Learning course, it is imperative to acknowledge the formidable challenges in developing the underlying architecture of these models. The computational demands for training from scratch pose significant obstacles in terms of time and cost. While tools such as Google Colab offer a solution, the complexity of creating a neural network from scratch necessitates a judicious approach.\n",
    "\n",
    "This leads us to the pragmatic choice of utilizing pretrained models. Acknowledging the challenges of constructing architectures from the ground up, this approach aims not only to streamline trading processes but also to enhance overall quality of life. By aligning with the fundamental principles of scientific and general progress, this term paper seeks to make our existence not only more efficient and productive but also undeniably better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563b0c39-1144-4dfd-aae8-cf02cd7f48a6",
   "metadata": {},
   "source": [
    "# 1. BERT - Bidirectional Encoder Representation from Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e302974f-ad70-4f18-aaec-6059ebf2f995",
   "metadata": {},
   "source": [
    "BERT enables context awareness for sentences. It is one of the most popular state of the art text embedding model published by Google. BERT has caused a revolution in the world of NLP by providing superior results on many NLP tasks, such as question answering, text generation, sentence classification, and many more compared to other methods.One of the reasons BERT is more successful is that it uses a context based embedding model. \n",
    "\n",
    "Consider the example below:\n",
    "\n",
    "**Sentence 1: The python ate the rabbit**\n",
    "\n",
    "**Sentence 2: Python is one of the most popular programming languages**\n",
    "\n",
    "Without context, the word python would have the same meaning in both sentences. BERT looks at the sentence and figures out what words python is related to in the sentence, and will create embedding of the word python based on the context. BERT does this by using transformers, which is a state of the art deep learning architecture, that is mostly used for Natural language processing. The architecture uses encoder-decoder paradigm.\n",
    "\n",
    "The encoder takes the input sentence and learns its representation and then sends the representation to the decoder. The decoder generates the output sentence. The transformer architecture uses many layers of encoders to generate the representation. BERT can be thought of as a transformer, but only with encoders. BERT has different configurations based on how many encoder layers it uses.\n",
    "\n",
    "<img src=\"./IMG/BERT_Arhitecture.PNG\" width=\"800\">\n",
    "\n",
    "The BERT model is pretrained on a large corpus of words. What is pretraining? Pretraining is when we train a model with a huge dataset and potentially a very large number of parameters for a particular task and save the trained model. For any new task, instead of initializing a new model with random weights, we will initialize it with the weights of the trained model, and adjust the weights for the new task. This is helpful since for our work we may not have easy access to huge volumes of training data, and we will save a lot of time and resources that were spent on training the model. \n",
    "\n",
    "The BERT model is pre-trained using two tasks: masked language modelling and next sentence prediction. BERT models have been trained on BookCorpus and English Wikipedia, which have in total more than 3.5 Billion words. Many domain specific models have emerged using BERT as the base and are being used for NLP tasks. Some of them are: FinBERT for Finance, BioBERT for Biomedical, VideoBERT for Video captioning categorization, ClinicalBERT for hospitals, and many more continue to evolve. If you are looking for cutting edge, deep learning pre-trained models for any domain, it would be worth researching to see if a DomainBERT model for that area exists.\n",
    "\n",
    "### 1.1 FinBERT\n",
    "\n",
    "FinBERT is a language model based on BERT. It further trains the BERT model for financial data. The additional training corpus is a set of 1.8M Reuters’ news articles and Financial PhraseBank.The main sentiment analysis dataset used is Financial PhraseBank which consists of 4845 English sentences selected randomly from financial news found on LexisNexis database. These sentences then were annotated by 16 people with backgrounds in finance and business.\n",
    "\n",
    "<img src=\"./IMG/FinBERT.PNG\" width=\"800\">\n",
    "\n",
    "### 1.2 Importance of sentiment analysi in Finance\n",
    "\n",
    "Sentiment analysis, meanwhile, is a very common task in NLP that aims to assign a \"feeling\" or an \"emotion\" to text. Typically, it predicts whether the sentiment is positive, negative, or neutral.\n",
    "You often see sentiment analysis around social media responses to hot-button issues or to determine the success of an ad campaign. But it's promising in the financial domain as changes in sentiment around a company could help predict a rise or fall in that company's stock. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "23317e77-0314-42b5-811f-dacb2a35f374",
   "metadata": {},
   "source": [
    "### 1.3 Brief implementation\n",
    "\n",
    "FinBERT implementation is reliant on Hugging Face’s pytorch_pretrained_bert library and their implementation of BERT for sequence classification tasks.In order to demonstrate FinBert in action, I will use a financial news dataset from [Kaggle](https://www.kaggle.com/datasets/notlucasp/financial-news-headlines).I have used Google colab to run this code to avoid making new enviroment just to show a qick demo of the FinBERT in action.\n",
    "\n",
    "Here is the result of the sentiment analysis:\n",
    "\n",
    "The code is in the **[FinBERT_implementation](./FinBERT_implementation.ipynb)**\n",
    "\n",
    "<img src=\"./IMG/FinBERT_Analysis.PNG\" width=\"500\">\n",
    "\n",
    "We manage it to do it in 20 lines of code and without any training from us.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "FinBERT makes the job of sentiment analysis for financial feeds very easy, and a lot more accurate. The heavy lifting for training and testing a model on a very large financial corpus has already been done by the researchers, and the model has been made public by Hugging Face. The rest of us can simply use it with very few lines of code to get fairly accurate results for financial sentiment analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f166591-c0dc-4fbd-a28d-67d2e3ebf648",
   "metadata": {},
   "source": [
    "# 2. Trading Bot implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1ace40-7379-43ea-a6f3-090319e54bb3",
   "metadata": {},
   "source": [
    "Some of the world's largest hedge funds are driven by algorithmic trading, outperforming the market through the utilization of cutting-edge algorithms. Now, let's endeavor to create our own.\n",
    "\n",
    "The initial step involves constructing our foundational bot. We will use [Alpaca trading API](https://alpaca.markets/) where we will acutaly test the bot.\n",
    "\n",
    "*note: the code implementation will be in Visual Studio Code, but i will provide parts of the code in the notebook and explain them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d410dec-3884-423f-8ef9-b1dad3ba4aba",
   "metadata": {},
   "source": [
    "### 2.1 Instaling dependencies and testing the first trade\n",
    "\n",
    "Firstly, let's set up the necessary dependencies. Ensure you have a virtual environment with Python 3.10 installed, along with the following libraries: lumibot, timedelta, alpaca-trade-api, torch, torchvision, torchaudio, and transformers. Additionally, you'll need to create an account on the [Alpaca trading API](https://alpaca.markets/), which is free, and obtain API keys for testing the bot.\n",
    "\n",
    "Lets see the first trade of our both:\n",
    "\n",
    "**[First Buy Code](./test_codes/First_Buy.py)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca41e28-694d-470b-bf7a-18aa98170fcb",
   "metadata": {},
   "source": [
    "<img src=\"./IMG/First_Buy.PNG\" width=\"1000\">\n",
    "\n",
    "Okay so we just bought 10 SPY contracts at a price of $ 469.49, at the start date that we have set and how the market is doing until the end date from the code.\n",
    "\n",
    "Other thing that we can check is the **[tearsheet](./logs/MLTrader_2024-01-25_12-43-32_tearsheet.html)** of our test buy, we can see more important information abouth the trade and the strategy.\n",
    "\n",
    "Now, we have a bot that randomly acquires a few shares intermittently, a strategy that may not be optimal for profit generation. It is essential to introduce position sizing and limits to effectively manage our funds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f41dd3-5271-43b0-8c55-ea527dff6c07",
   "metadata": {},
   "source": [
    "### 2.2 Position sizing and limits\n",
    "\n",
    "In determining our optimal position size, we employ the cash-at-risk method—a strategy that encapsulates the total monetary commitment made by an investor or trader to a specific trade. This metric represents the capital vulnerable to loss if the trade deviates from the expected outcome. In our approach, we've configured the parameter to be 50% of the available cash, allowing for dynamic adjustments based on risk appetite and trading preferences.\n",
    "\n",
    "Moving forward, our strategy involves the implementation of stop loss and take profit parameters—fundamental tools in trading for managing risk and securing profits. A Stop Loss order automatically sells a security when it reaches a specified price, curbing potential losses. Conversely, a Take Profit order automatically sells a security at a predetermined price to lock in profits. Both orders provide a systematic framework for risk management and profit capture, fostering a disciplined and strategic trading approach. They are set acordigly - Take Profit to 20% and Stop Loss to 5%\n",
    "\n",
    "After we created this parameter let's test the **[code](./test_codes/Position_Sizing.py)**\n",
    "\n",
    "<img src=\"./IMG/Sizing.PNG\" width=\"1000\">\n",
    "\n",
    "So we just bought 106 shares of the SPY index, which is half of our cash as we set in the cash-at-risk parameter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3352153b-dcc2-4099-8e7f-fc675b41b7c0",
   "metadata": {},
   "source": [
    "### 2.3 News\n",
    "\n",
    "Now that our bot is up and running, it's time to delve into the strategy and incorporate some cutting-edge Deep Learning elements. Forecasting market trends has historically been a laborious task, with traders dedicating countless hours to reading and analyzing news. This is where neural networks, specifically FinBERT, come into play. The objective is to extract insights from the news in the three days preceding our market actions and delegate the decision-making process to the model. FinBERT takes on the intricate task of deciphering the information and determining the optimal course of action.\n",
    "\n",
    "Let's explore specific instances of the model in action. In the  **[finbert_utils](./finbert_utils.py)** we delve into examples illustrating the model's reactions:\n",
    "\n",
    "For instance, consider news headlines like:\n",
    "\n",
    " **\"Markets exhibited a negative response to the latest developments!\", \"Traders expressed dissatisfaction with the unfolding events!\"**\n",
    "\n",
    "<img src=\"./IMG/Negative_News.PNG\" width=\"1000\">\n",
    "\n",
    "we are geting negative result with probability of 0.9989!\n",
    "\n",
    "On news headlines like:\n",
    "\n",
    " **\"Markets surged in enthusiastic response to the latest news!\", \"Traders celebrated as the market witnessed a favorable turn of events!\"**\n",
    "\n",
    " <img src=\"./IMG/Positive_News.PNG\" width=\"1000\">\n",
    "\n",
    " we've got 0.9003 positive probability.\n",
    "\n",
    " The third option available to us is a neutral stance, where we refrain from taking any specific action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d8a05a-a729-4d8a-b049-88892317ea40",
   "metadata": {},
   "source": [
    "### 2.4 Include the neural network\n",
    "\n",
    "Once we comprehend the operational dynamics of the FinBERT model, let's seamlessly integrate it into our **[code](./test_codes/Include_NN.py)** and subject it to a brief testing phase. This will provide us with valuable insights into its functionality and performance over a short timeframe.\n",
    "\n",
    "<img src=\"./IMG/NN_Test.PNG\" width=\"1000\">\n",
    "\n",
    "Fantastic! It's evident that the bot has executed some trades, and while we can anticipate preliminary results for this brief period, the next logical step involves fine-tuning the code. Let's make subtle adjustments and proceed to conduct a comprehensive full backtest to thoroughly evaluate the performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318f6943-ce57-47c8-bcb9-aaf469163083",
   "metadata": {},
   "source": [
    "### 2.5 Final Test\n",
    "\n",
    "The **[Final code](./tradingbot.py)**.\n",
    "\n",
    "At the heart of the strategy, the on_trading_iteration method takes center stage in each trading cycle. This critical function systematically assesses the prevailing sentiment and probability scores, guiding the decision-making process for buy or sell orders. When sentiment is notably positive, surpassing the 99.9% threshold in probability, a buy order is executed, employing a practical bracket strategy with take-profit and stop-loss parameters. Conversely, in instances of pronounced negative sentiment with a probability exceeding 99.9%, a sell order is initiated, mirroring the structured approach of the buy order. To maintain operational efficiency, the strategy intelligently manages the last executed trade, preventing the occurrence of redundant orders and ensuring a smooth and effective trading process.\n",
    "\n",
    "Let's engage in trading the SPY index over a span of four full years and analyze the resultant outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee824a3a-2410-4f5b-8fef-538121ca5504",
   "metadata": {},
   "source": [
    "# 3. Analysis\n",
    "\n",
    "**[trades](./logs/MLTrader_2024-01-27_13-26-40_trades.html)**\n",
    "\n",
    "<img src=\"./IMG/Full_BackTest.PNG\" width=\"1000\">\n",
    "\n",
    "Our strategy, despite its straightforward nature, is delivering impressive results. We've shown proficiency in shorting positions effectively during market downturns. A significant highlight occurred on August 15, when the portfolio reached a remarkable $537 836—more than five times our initial investment. This underscores the success of our uncomplicated approach, consistently generating substantial returns amid market fluctuations.\n",
    "\n",
    "Let's take a look of the **[tearsheet](./logs/MLTrader_2024-01-27_13-26-40_tearsheet.html)**\n",
    "\n",
    "The strategy's performance is underpinned by a set of key metrics, showcasing its efficacy and resilience in navigating financial markets.\n",
    "\n",
    "The Cumulative Return stands out as a remarkable achievement, with the strategy delivering an impressive 234.46%, significantly surpassing the SPY index at 56.41%. This metric serves as a testament to the strategy's ability to generate substantial returns over the specified period.\n",
    "\n",
    "Moving on to the CAGR (Compound Annual Growth Rate), the strategy demonstrates sustained growth with a robust figure of 23.8%, outshining the SPY's 8.23%. This underscores the strategy's proficiency in consistently compounding positive returns, providing a reliable indicator of its long-term performance.\n",
    "\n",
    "In terms of Risk Metrics, the strategy exhibits notable enhancements in risk management. It boasts a higher Sharpe ratio (0.64 vs. 0.32) and a lower maximum drawdown (-68.94% vs. -33.68%). Additionally, superior risk-adjusted performance is evident in the Sortino and Calmar ratios, emphasizing the strategy's adeptness in balancing risk and reward.\n",
    "\n",
    "Embracing a dynamic trading approach, the strategy introduces higher Volatility at 54.15%, compared to SPY's 20.71%. This deliberate acceptance of volatility signifies the strategy's adaptability and responsiveness to market dynamics.\n",
    "\n",
    "Despite encountering extended drawdown periods, the strategy showcases resilience in its Drawdowns. The recovery factor of 2.71 surpasses SPY's recovery factor of 1.63, highlighting the strategy's ability to recover and rebound effectively after facing significant downturns.\n",
    "\n",
    "Consistency characterizes the strategy's Yearly Returns, with notable outperformance observed in 2020 and 2022. Even during the worst-performing year for the strategy (-2.50%), it surpasses SPY's worst year (-18.16%), demonstrating its ability to weather challenging market conditions.\n",
    "\n",
    "The Benchmark Comparison reveals a consistent outperformance trend over multiple years, as highlighted in the EOY (End of Year) Returns vs. Benchmark analysis. This indicates the strategy's capacity to consistently surpass benchmark performance metrics.\n",
    "\n",
    "Insights gleaned from the Drawdown Analysis underscore the strategy's adeptness in rebounding effectively after facing significant downturns. This analysis provides valuable insights into the strategy's recovery mechanisms.\n",
    "\n",
    "A distinctive performance pattern emerges in the Correlation metric, with a low correlation of -21.33%, suggesting potential diversification benefits. This unique correlation signifies that the strategy operates independently, offering a degree of insulation from broader market trends.\n",
    "\n",
    "Finally, the strategy's sustained positive performance over 3-year, 5-year, and 10-year Multi-Year Performance underscores its resilience and ability to deliver favorable outcomes over extended periods. This long-term perspective solidifies the strategy's position as a reliable performer in varying market conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e208106-8f30-4e92-be91-5a7f85bdcd48",
   "metadata": {},
   "source": [
    "# Refferences\n",
    "[1] Financial Sentiment Analysis on Stock Market Headlines With FinBERT & HuggingFace by Ivan Goncharov, Jul 28, 2023\n",
    "\n",
    "[2] Financial Sentiment Analysis using FinBert by Praveen Purohit, Dec, 2021"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trader",
   "language": "python",
   "name": "trader"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
